#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ä»¶æ•´ç†å¼•æ“
æ ¸å¿ƒæ•´ç†é€»è¾‘ï¼Œåè°ƒå„ç»„ä»¶å®Œæˆæ–‡ä»¶æ•´ç†ä»»åŠ¡
"""

import os
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, asdict

from config import get_config
from logger import get_logger, OperationTimer
from file_analyzer import FileAnalyzer
from duplicate_detector import DuplicateDetector
from backup_manager import get_backup_manager, BackupRecord
from directory_analyzer import DirectoryAnalyzer
from utils import (
    get_file_info, safe_move_file, validate_directory_permissions,
    get_available_disk_space, format_file_size
)


@dataclass
class OrganizeOperation:
    """æ•´ç†æ“ä½œè®°å½•"""
    source_path: str
    target_path: str
    action: str  # move, copy, skip, duplicate
    file_size: int
    timestamp: datetime
    success: bool
    session_id: Optional[str] = None
    backup_path: Optional[str] = None
    file_hash: Optional[str] = None
    error_message: Optional[str] = None


@dataclass
class OrganizeStatistics:
    """æ•´ç†ç»Ÿè®¡ä¿¡æ¯"""
    total_files: int = 0
    processed_files: int = 0
    moved_files: int = 0
    skipped_files: int = 0
    duplicate_files: int = 0
    error_files: int = 0
    total_size: int = 0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    
    @property
    def duration(self) -> float:
        """æ€»è€—æ—¶ï¼ˆç§’ï¼‰"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return 0.0
    
    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡"""
        if self.processed_files == 0:
            return 0.0
        return (self.moved_files + self.skipped_files) / self.processed_files


class FileOrganizer:
    """æ–‡ä»¶æ•´ç†å¼•æ“ä¸»ç±»"""
    
    def __init__(self, target_directory: str, config_file: Optional[str] = None):
        """
        åˆå§‹åŒ–æ–‡ä»¶æ•´ç†å™¨
        
        Args:
            target_directory: ç›®æ ‡æ•´ç†ç›®å½•
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.target_directory = Path(target_directory).resolve()
        self.config = get_config(config_file)
        self.logger = get_logger()
        
        # æ›´æ–°é…ç½®ä¸­çš„ç›®æ ‡ç›®å½•
        self.config.set('target_directory', str(self.target_directory))
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.analyzer = FileAnalyzer()
        self.duplicate_detector = DuplicateDetector()
        self.backup_manager = get_backup_manager()
        self.directory_analyzer = DirectoryAnalyzer()
        
        # æ“ä½œè®°å½•
        self.operations: List[OrganizeOperation] = []
        self.statistics = OrganizeStatistics()
        self.session_id: Optional[str] = None
        
        # éªŒè¯ç›®æ ‡ç›®å½•
        self._validate_target_directory()
    
    def _validate_target_directory(self):
        """éªŒè¯ç›®æ ‡ç›®å½•"""
        has_permission, errors = validate_directory_permissions(self.target_directory)
        
        if not has_permission:
            error_msg = f"ç›®æ ‡ç›®å½•æƒé™ä¸è¶³: {'; '.join(errors)}"
            self.logger.error(error_msg)
            raise PermissionError(error_msg)
        
        self.logger.info(f"ç›®æ ‡ç›®å½•éªŒè¯é€šè¿‡: {self.target_directory}")
    
    def _deduplicate_files(self, file_paths: List[Path]) -> Tuple[List[Path], Dict[str, str]]:
        """
        å»é‡æ–‡ä»¶é€Ÿå¹³
        
        Args:
            file_paths: åŸå§‹æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            (ä¸é‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨, MD5å“ˆå¸Œåˆ°è·¯å¾„çš„æ˜ å°„)
        """
        self.logger.info(f"å¼€å§‹å»é‡: æ‰«æ {len(file_paths)} ä¸ªæ–‡ä»¶")
        
        hash_map: Dict[str, str] = {}  # MD5å“ˆå¸Œ -> æ–‡ä»¶è·¯å¾„
        duplicate_map: Dict[str, str] = {}  # é‡å¤æ–‡ä»¶ -> åŸä¾‹
        unique_files: List[Path] = []
        
        for file_path in file_paths:
            try:
                # è®¡ç®—MD5å“ˆå¸Œ
                file_hash = self._calculate_file_hash(file_path)
                if not file_hash:
                    unique_files.append(file_path)
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if file_hash in hash_map:
                    # è¿™æ˜¯ä¸€ä¸ªé‡å¤æ–‡ä»¶
                    duplicate_map[str(file_path)] = hash_map[file_hash]
                    self.logger.debug(f"æ£€æµ‹åˆ°é‡å¤: {file_path.name} â† {Path(hash_map[file_hash]).name}")
                else:
                    # æ–°çš„ä¸é‡æ–‡ä»¶
                    hash_map[file_hash] = str(file_path)
                    unique_files.append(file_path)
                    
            except Exception as e:
                self.logger.warning(f"å»é‡æ£€æŸ¥å¤±è´¥ {file_path}: {str(e)}")
                unique_files.append(file_path)  # å¤±è´¥ä¼šä¿ç•™æ–‡ä»¶
        
        self.logger.info(f"å»é‡ç»“æœ: åŸå§‹ {len(file_paths)} -> ä¸é‡ {len(unique_files)} ä¸ªæ–‡ä»¶ã€æ£€æµ‹åˆ° {len(duplicate_map)} ä¸ªé‡å¤")
        return unique_files, duplicate_map
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """
        è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            MD5å“ˆå¸Œå­—ç¬¦ä¸²
        """
        import hashlib
        try:
            md5 = hashlib.md5()
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(8192), b''):
                    md5.update(chunk)
            return md5.hexdigest()
        except Exception as e:
            self.logger.warning(f"è®¡ç®—å“ˆå¸Œå¤±è´¥ {file_path}: {str(e)}")
            return ""
    
    def _group_by_content_relevance(self, analyzed_files: List[Dict]) -> Dict[str, List[Dict]]:
        """
        æ ¹æ®å†…å®¹ç›¸å…³æ€§åˆ†ç»„æ–‡ä»¶
        
        Args:
            analyzed_files: åˆ†æå®Œçš„æ–‡ä»¶åˆ—è¡¨
            
        Returns:
            ä¸»é¢˜ -> æ–‡ä»¶åˆ—è¡¨çš„å­—å…¸
        """
        self.logger.info(f"å¼€å§‹æŒ‰å†…å®¹ç›¸å…³æ€§åˆ†ç»„: {len(analyzed_files)} ä¸ªæ–‡ä»¶")
        
        groups: Dict[str, List[Dict]] = {}
        project_files: List[Dict] = []  # é¡¹ç›®æ–‡ä»¶ä¸“é¡¹
        
        for file_analysis in analyzed_files:
            # ä¼˜å…ˆä¿æŠ¤é¡¹ç›®æ–‡ä»¶
            if file_analysis.get('project_info', {}).get('should_protect'):
                project_files.append(file_analysis)
                continue
            
            # æ ¹æ®è¯­ä¹‰ä¸»é¢˜åˆ†ç»„
            semantic_theme = file_analysis.get('semantic_theme', 'uncategorized')
            if semantic_theme not in groups:
                groups[semantic_theme] = []
            groups[semantic_theme].append(file_analysis)
        
        # é¡¹ç›®æ–‡ä»¶å•ç‹¬åˆ†ç»„
        if project_files:
            groups['ProjectFiles'] = project_files
        
        self.logger.info(f"åˆ†ç»„å®Œæˆ: {len(groups)} ä¸ªä¸»é¢˜ç»„")
        for theme, files in groups.items():
            self.logger.info(f"  {theme}: {len(files)} ä¸ªæ–‡ä»¶")
        
        return groups
    
    def _calculate_success_metrics(self, original_count: int, unique_count: int, organized_count: int, 
                                 backup_count: int, protected_count: int = 0) -> Dict[str, any]:
        """
        è®¡ç®—æˆåŠŸåˆ¤å®šæŒ‡æ ‡
        
        Args:
            original_count: åŸå§‹æ–‡ä»¶æ•°
            unique_count: å»é‡åæ–‡ä»¶æ•°
            organized_count: æ•´ç†åæ–‡ä»¶æ•°
            backup_count: å¤‡ä»½æ–‡ä»¶æ•°
            protected_count: å—ä¿æŠ¤çš„é¡¹ç›®æ–‡ä»¶æ•°
            
        Returns:
            æˆåŠŸæŒ‡æ ‡å­—å…¸
        """
        # åˆ¤æ–­æˆåŠŸæ¡ä»¶
        success = (organized_count == unique_count) and (backup_count >= 0)
        
        metrics = {
            'original_count': original_count,
            'unique_count': unique_count,
            'organized_count': organized_count,
            'backup_count': backup_count,
            'protected_count': protected_count,
            'duplicates_removed': original_count - unique_count,
            'success': success,
            'message': f"æ•´ç†{'' if success else 'ä¸'}'æˆåŠŸ: åŸå§‹{original_count} â†’ ä¸é‡{unique_count} â†’ æ•´ç†{organized_count}"
        }
        
        return metrics
    
    def enhanced_analyze_directory(self) -> Dict[str, any]:
        """
        å¢å¼ºç›®å½•åˆ†æï¼šé›†æˆtreeå‘½ä»¤è¾“å‡ºåˆ†æå’Œå¤šæ–¹æ¡ˆç”Ÿæˆ
        
        Returns:
            å¢å¼ºåˆ†æç»“æœ
        """
        self.logger.operation_start("å¢å¼ºç›®å½•åˆ†æ", str(self.target_directory))
        
        try:
            # æ‰§è¡Œå¢å¼ºç›®å½•åˆ†æ
            analysis_result = self.directory_analyzer.analyze_with_tree(str(self.target_directory))
            
            # ä¿å­˜åˆ†ææŠ¥å‘Š
            report_path = self.directory_analyzer.save_analysis_report(analysis_result, 'markdown')
            
            # ç”Ÿæˆæ–¹æ¡ˆé¢„è§ˆ
            scheme_previews = []
            for scheme in analysis_result['schemes']:
                preview = self._generate_scheme_preview(scheme, analysis_result['statistics'])
                scheme_previews.append(preview)
            
            result = {
                'success': True,
                'message': 'å¢å¼ºç›®å½•åˆ†æå®Œæˆ',
                'directory': str(self.target_directory),
                'analysis_result': analysis_result,
                'scheme_previews': scheme_previews,
                'recommended_scheme': analysis_result['recommendation'],
                'report_path': report_path
            }
            
            self.logger.operation_complete(
                "å¢å¼ºç›®å½•åˆ†æ",
                0,
                {
                    'æ€»æ–‡ä»¶æ•°': analysis_result['statistics']['total_files'],
                    'æ–¹æ¡ˆæ•°é‡': len(analysis_result['schemes']),
                    'æ¨èæ–¹æ¡ˆ': analysis_result['recommendation']['name']
                }
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"å¢å¼ºç›®å½•åˆ†æå¤±è´¥: {str(e)}")
            return {
                'success': False,
                'message': f"åˆ†æå¤±è´¥: {str(e)}"
            }
    
    def _generate_scheme_preview(self, scheme: 'AnalysisScheme', statistics: Dict) -> Dict[str, any]:
        """ç”Ÿæˆæ–¹æ¡ˆé¢„è§ˆ"""
        return {
            'scheme_id': scheme.scheme_id,
            'name': scheme.name,
            'description': scheme.description,
            'confidence': scheme.confidence,
            'risk_level': scheme.risk_level,
            'estimated_moves': scheme.estimated_moves,
            'estimated_time': scheme.estimated_time,
            'details': scheme.details,
            'preview_stats': {
                'files_to_move': min(scheme.estimated_moves, statistics['total_files']),
                'directories_to_create': len(scheme.details.get('categories', [])),
                'space_efficiency': 'é«˜' if scheme.confidence > 0.8 else 'ä¸­' if scheme.confidence > 0.6 else 'ä½'
            }
        }
    
    def smart_organize(self, analyze_only: bool = False, plan_only: bool = False, 
                      execute_plan: Optional[str] = None, dry_run: bool = False) -> Dict[str, any]:
        """
        å››é˜¶æ®µæ™ºèƒ½æ•´ç†
        
        Args:
            analyze_only: ä»…æ‰§è¡Œåˆ†æé˜¶æ®µ
            plan_only: ä»…ç”Ÿæˆæ•´ç†æ–¹æ¡ˆ
            execute_plan: æ‰§è¡ŒæŒ‡å®šçš„æ•´ç†æ–¹æ¡ˆæ–‡ä»¶
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼
            
        Returns:
            æ•´ç†ç»“æœå­—å…¸
        """
        self.logger.operation_start("å››é˜¶æ®µæ™ºèƒ½æ•´ç†", str(self.target_directory))
        
        try:
            # é˜¶æ®µä¸€ï¼šå¯åŠ¨å¤‡ä»½ä¼šè¯
            self.session_id = self.backup_manager.start_session(str(self.target_directory))
            self.logger.info(f"ğŸ“‹ é˜¶æ®µä¸€ï¼šå¯åŠ¨å¤‡ä»½ä¼šè¯ {self.session_id}")
            
            # é˜¶æ®µäºŒï¼šæ–‡ä»¶åˆ†æ
            with OperationTimer("æ–‡ä»¶åˆ†æ", self.logger):
                file_paths = self._scan_files()
                self.statistics.total_files = len(file_paths)
                self.logger.info(f"ğŸ“Š æ‰«æåˆ° {len(file_paths)} ä¸ªæ–‡ä»¶")
                
                if not file_paths:
                    self.logger.warning("ç›®æ ‡ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶")
                    self.backup_manager.cancel_session()
                    return self._create_result_dict(success=True, message="ç›®å½•ä¸ºç©º")
                
                analyzed_files = self._analyze_files(file_paths)
                self.logger.info(f"ğŸ” æ–‡ä»¶åˆ†æå®Œæˆï¼Œè¯†åˆ«å‡º {len(analyzed_files)} ä¸ªæ–‡ä»¶çš„ç‰¹å¾")
            
            if analyze_only:
                self.backup_manager.cancel_session()
                return self._create_analysis_result(analyzed_files)
            
            # é˜¶æ®µä¸‰ï¼šç”Ÿæˆæ•´ç†æ–¹æ¡ˆ
            with OperationTimer("AIå†³ç­–", self.logger):
                plan = self._generate_smart_plan(analyzed_files, file_paths)
                self.logger.info(f"ğŸ§  AIå†³ç­–å®Œæˆï¼Œç”Ÿæˆæ•´ç†æ–¹æ¡ˆ")
                
                if plan_only:
                    plan_file = self._save_plan(plan)
                    self.backup_manager.cancel_session()
                    return self._create_plan_result(plan, plan_file)
                
                if execute_plan:
                    plan = self._load_plan(execute_plan)
            
            # é˜¶æ®µå››ï¼šå®‰å…¨æ‰§è¡Œ
            with OperationTimer("å®‰å…¨æ‰§è¡Œ", self.logger):
                self._execute_smart_plan(plan, dry_run)
            
            # å®Œæˆä¼šè¯
            self.backup_manager.complete_session({
                'moved_files': self.statistics.moved_files,
                'skipped_files': self.statistics.skipped_files,
                'duplicate_files': self.statistics.duplicate_files,
                'error_files': self.statistics.error_files
            })
            
            # å®Œæˆç»Ÿè®¡
            self.statistics.end_time = datetime.now()
            
            # ç”Ÿæˆç»“æœ
            result = self._create_smart_result(
                success=True,
                message=f"æ™ºèƒ½æ•´ç†å®Œæˆï¼Œå¤„ç† {self.statistics.processed_files} ä¸ªæ–‡ä»¶",
                plan=plan,
                dry_run=dry_run
            )
            
            self.logger.operation_complete(
                "å››é˜¶æ®µæ™ºèƒ½æ•´ç†", 
                self.statistics.duration,
                {
                    'å¤„ç†æ–‡ä»¶': self.statistics.processed_files,
                    'ç§»åŠ¨æ–‡ä»¶': self.statistics.moved_files,
                    'è·³è¿‡æ–‡ä»¶': self.statistics.skipped_files,
                    'é‡å¤æ–‡ä»¶': self.statistics.duplicate_files,
                    'é”™è¯¯æ–‡ä»¶': self.statistics.error_files
                }
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½æ•´ç†è¿‡ç¨‹å‡ºé”™: {str(e)}")
            if self.session_id:
                self.backup_manager.cancel_session()
            return self._create_result_dict(
                success=False,
                message=f"æ•´ç†å¤±è´¥: {str(e)}"
            )
    
    def _scan_files(self) -> List[Path]:
        """æ‰«æç›®æ ‡ç›®å½•ä¸­çš„æ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        exclude_patterns = self.config.get('exclude_patterns', [])
        exclude_dirs = self.config.get('exclude_directories', [])
        
        file_paths = []
        skipped_items = []
        error_items = []
        
        self.logger.info(f"å¼€å§‹æ·±åº¦æ‰«æç›®å½•: {self.target_directory}")
        
        try:
            # ä½¿ç”¨os.walkè¿›è¡Œæ›´å¯é çš„æ‰«æ
            for root, dirs, files in os.walk(self.target_directory, followlinks=True):
                root_path = Path(root)
                
                # æ£€æŸ¥ç›®å½•æ˜¯å¦åº”è¯¥è¢«æ’é™¤
                should_skip_dir = False
                for exclude_dir in exclude_dirs:
                    if exclude_dir in root_path.parts:
                        should_skip_dir = True
                        skipped_items.append((root_path, "excluded_directory"))
                        break
                
                if should_skip_dir:
                    dirs.clear()  # ä¸è¿›å…¥å­ç›®å½•
                    continue
                
                # å¤„ç†æ–‡ä»¶
                for file_name in files:
                    file_path = root_path / file_name
                    
                    try:
                        # æ£€æŸ¥åŸºæœ¬æ’é™¤æ¨¡å¼
                        should_exclude = False
                        for pattern in exclude_patterns:
                            if file_path.match(pattern):
                                should_exclude = True
                                skipped_items.append((file_path, f"pattern_match:{pattern}"))
                                break
                        
                        # æ£€æŸ¥æ–‡ä»¶å±æ€§
                        if not should_exclude:
                            try:
                                stat_info = file_path.stat()
                                # è·³è¿‡éå¸¸å°çš„æ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯ç³»ç»Ÿæ–‡ä»¶ï¼‰
                                if stat_info.st_size < 1:  # å°äº1å­—èŠ‚
                                    should_exclude = True
                                    skipped_items.append((file_path, "zero_size"))
                                # è·³è¿‡éšè—æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
                                elif file_name.startswith('.') and not self.config.get('include_hidden', False):
                                    should_exclude = True
                                    skipped_items.append((file_path, "hidden_file"))
                            except (OSError, PermissionError) as e:
                                error_items.append((file_path, f"stat_error:{str(e)}"))
                                should_exclude = True
                        
                        if not should_exclude:
                            file_paths.append(file_path)
                            
                    except Exception as e:
                        error_items.append((file_path, f"processing_error:{str(e)}"))
                        self.logger.debug(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {str(e)}")
                        continue
                
                # é™åˆ¶é€’å½’æ·±åº¦ï¼ˆå¯é…ç½®ï¼‰
                max_depth = self.config.get('max_scan_depth', 10)
                current_depth = len(root_path.relative_to(self.target_directory).parts)
                if current_depth >= max_depth:
                    dirs.clear()
            
            # è®°å½•æ‰«æç»Ÿè®¡
            self.logger.info(f"æ‰«æå®Œæˆç»Ÿè®¡:")
            self.logger.info(f"  - å‘ç°æ–‡ä»¶: {len(file_paths)} ä¸ª")
            self.logger.info(f"  - è·³è¿‡é¡¹ç›®: {len(skipped_items)} ä¸ª")
            self.logger.info(f"  - é”™è¯¯é¡¹ç›®: {len(error_items)} ä¸ª")
            
            # å¦‚æœå¯ç”¨äº†è¯¦ç»†æ—¥å¿—ï¼Œè®°å½•è·³è¿‡çš„æ–‡ä»¶
            if self.config.get('verbose_scan_log', False) and skipped_items:
                self.logger.debug("è·³è¿‡çš„æ–‡ä»¶è¯¦æƒ…:")
                for item, reason in skipped_items[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    self.logger.debug(f"  {item} - {reason}")
                if len(skipped_items) > 10:
                    self.logger.debug(f"  ... è¿˜æœ‰ {len(skipped_items) - 10} ä¸ªè·³è¿‡é¡¹")
            
            # è®°å½•é”™è¯¯æ–‡ä»¶
            if error_items:
                self.logger.warning(f"æ‰«æè¿‡ç¨‹ä¸­é‡åˆ° {len(error_items)} ä¸ªé”™è¯¯æ–‡ä»¶")
                for item, error in error_items[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                    self.logger.debug(f"  {item} - {error}")
            
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶æ‰«æå¤±è´¥: {str(e)}")
            raise
        
        return file_paths
    
    def _analyze_files(self, file_paths: List[Path]) -> List[Dict[str, any]]:
        """åˆ†ææ–‡ä»¶"""
        return self.analyzer.batch_analyze(file_paths)
    
    def _detect_duplicates(self, file_paths: List[Path]) -> List[List[Path]]:
        """æ£€æµ‹é‡å¤æ–‡ä»¶"""
        method = self.config.get('duplicate_detection_method', 'smart')
        return self.duplicate_detector.detect_duplicates(file_paths, method)
    
    def _process_files(self, analyzed_files: List[Dict], 
                      duplicates: List[List[Path]], dry_run: bool):
        """å¤„ç†æ–‡ä»¶"""
        # åˆ›å»ºé‡å¤æ–‡ä»¶æ˜ å°„
        duplicate_map = {}
        for group in duplicates:
            keeper, candidates = self.duplicate_detector.resolve_duplicates(group)
            for candidate in candidates:
                duplicate_map[str(candidate)] = str(keeper)
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for file_analysis in analyzed_files:
            try:
                self._process_single_file(file_analysis, duplicate_map, dry_run)
            except Exception as e:
                self.logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_analysis['file_info']['path']}: {str(e)}")
                self._record_operation(
                    source_path=file_analysis['file_info']['path'],
                    target_path="",
                    action="error",
                    file_size=file_analysis['file_info']['size'],
                    success=False,
                    error_message=str(e)
                )
                self.statistics.error_files += 1
    
    def _process_single_file(self, file_analysis: Dict, 
                           duplicate_map: Dict[str, str], dry_run: bool):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        file_info = file_analysis['file_info']
        file_path = Path(file_info['path'])
        
        self.statistics.processed_files += 1
        self.statistics.total_size += file_info['size']
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ–‡ä»¶
        if str(file_path) in duplicate_map:
            self._handle_duplicate_file(file_path, duplicate_map[str(file_path)], dry_run)
            return
        
        # ç¡®å®šç›®æ ‡è·¯å¾„
        target_path = self._determine_target_path(file_analysis)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç§»åŠ¨
        if file_path == target_path:
            self._record_operation(
                source_path=str(file_path),
                target_path=str(target_path),
                action="skip",
                file_size=file_info['size'],
                success=True
            )
            self.statistics.skipped_files += 1
            return
        
        # æ‰§è¡Œç§»åŠ¨
        self._move_file(file_path, target_path, dry_run)
    
    def _handle_duplicate_file(self, file_path: Path, keeper_path: str, dry_run: bool):
        """å¤„ç†é‡å¤æ–‡ä»¶"""
        strategy = self.config.get('duplicate_strategy', 'keep_newest')
        
        if strategy == 'keep_all':
            # ä¿ç•™æ‰€æœ‰æ–‡ä»¶ï¼Œæ­£å¸¸å¤„ç†
            return
        
        # ç§»åŠ¨é‡å¤æ–‡ä»¶åˆ°éš”ç¦»ç›®å½•
        isolation_dir = self.target_directory / 'Duplicates_Isolated'
        target_path = isolation_dir / file_path.name
        
        self._move_file(file_path, target_path, dry_run, action="duplicate")
        self.statistics.duplicate_files += 1
    
    def _determine_target_path(self, file_analysis: Dict) -> Path:
        """ç¡®å®šæ–‡ä»¶çš„ç›®æ ‡è·¯å¾„"""
        file_info = file_analysis['file_info']
        file_path = Path(file_info['path'])
        
        # è·å–å»ºè®®çš„åˆ†ç±»ç›®å½•
        category = file_analysis['suggested_category']
        target_dir = self.target_directory / category
        
        # åº”ç”¨å‘½åè§„èŒƒ
        naming_pattern = self.config.get_naming_pattern(file_info['suffix'])
        new_filename = self._apply_naming_pattern(file_path.name, file_analysis, naming_pattern)
        
        return target_dir / new_filename
    
    def _apply_naming_pattern(self, filename: str, file_analysis: Dict, pattern: str) -> str:
        """åº”ç”¨å‘½åè§„èŒƒ"""
        file_info = file_analysis['file_info']
        naming_features = file_analysis['naming_features']
        
        # æ›¿æ¢å ä½ç¬¦
        result = pattern
        
        # åŸºæœ¬æ›¿æ¢
        result = result.replace('{filename}', filename)
        result = result.replace('{category}', file_analysis['suggested_category'])
        
        # æ—¶é—´ç›¸å…³æ›¿æ¢
        now = datetime.now()
        result = result.replace('{year}', now.strftime('%Y'))
        result = result.replace('{month}', now.strftime('%m'))
        result = result.replace('{day}', now.strftime('%d'))
        
        # æ–‡ä»¶åç›¸å…³æ›¿æ¢
        result = result.replace('{name}', file_info['stem'])
        result = result.replace('{ext}', file_info['suffix'][1:] if file_info['suffix'] else '')
        
        # å‘½åç‰¹å¾æ›¿æ¢
        if naming_features['has_date'] and naming_features['date_info']:
            date_info = naming_features['date_info']
            result = result.replace('{file_year}', str(date_info['year']))
            result = result.replace('{file_month}', f"{date_info['month']:02d}")
            result = result.replace('{file_day}', f"{date_info['day']:02d}")
        
        if naming_features['has_version'] and naming_features['version_info']:
            result = result.replace(
                '{version}', 
                naming_features['version_info']['version_number']
            )
        
        if naming_features['has_project'] and naming_features['project_info']:
            result = result.replace(
                '{project}', 
                naming_features['project_info']['likely_project'] or 'unknown'
            )
        
        return result
    
    def _move_file(self, source_path: Path, target_path: Path, 
                   dry_run: bool, action: str = "move"):
        """ç§»åŠ¨æ–‡ä»¶ï¼ˆé€‚åº”æ–°çš„ç§»åŠ¨-å¤åˆ¶å¤‡ä»½ç­–ç•¥ï¼‰"""
        try:
            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            target_path.parent.mkdir(parents=True, exist_ok=True)
            
            backup_path = None
            file_hash = None
            
            if dry_run:
                # è¯•è¿è¡Œæ¨¡å¼ï¼šåªè®°å½•æ“ä½œï¼Œä¸å®é™…ç§»åŠ¨
                self.logger.info(f"[è¯•è¿è¡Œ] {action}: {source_path} â†’ {target_path}")
                success = True
                error_msg = None
            else:
                # å®é™…æ“ä½œï¼šä»å¤‡ä»½ä½ç½®å¤åˆ¶æ–‡ä»¶åˆ°ç›®æ ‡ä½ç½®
                if self.session_id and action == "move":
                    # æŸ¥æ‰¾è¯¥æ–‡ä»¶çš„å¤‡ä»½è®°å½•
                    backup_record = self._find_backup_record(str(source_path))
                    if backup_record:
                        backup_path = backup_record.backup_path
                        file_hash = backup_record.file_hash
                        
                        # ä»å¤‡ä»½ä½ç½®å¤åˆ¶æ–‡ä»¶åˆ°æ–°ä½ç½®
                        if Path(backup_path).exists():
                            shutil.copy2(str(backup_path), str(target_path))
                            success = True
                            self.logger.debug(f"ä»å¤‡ä»½å¤åˆ¶æ–‡ä»¶: {backup_path} â†’ {target_path}")
                        else:
                            success = False
                            error_msg = f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_path}"
                            self.logger.error(error_msg)
                    else:
                        # å¦‚æœæ‰¾ä¸åˆ°å¤‡ä»½è®°å½•ï¼Œå°è¯•ç›´æ¥å¤åˆ¶åŸå§‹æ–‡ä»¶
                        if source_path.exists():
                            shutil.copy2(str(source_path), str(target_path))
                            success = True
                            self.logger.debug(f"ç›´æ¥å¤åˆ¶æ–‡ä»¶: {source_path} â†’ {target_path}")
                        else:
                            success = False
                            error_msg = f"åŸå§‹æ–‡ä»¶å’Œå¤‡ä»½éƒ½ä¸å­˜åœ¨: {source_path}"
                            self.logger.error(error_msg)
                else:
                    # éæ•´ç†æ“ä½œæˆ–å…¶ä»–æƒ…å†µ
                    backup_enabled = self.config.get('backup_enabled', True)
                    success = safe_move_file(source_path, target_path, backup_enabled)
                    error_msg = None
                    
                if success:
                    self.logger.file_processed(action, str(source_path), str(target_path), 
                                             Path(backup_path or source_path).stat().st_size)
            
            self._record_operation(
                source_path=str(source_path),
                target_path=str(target_path),
                action=action,
                file_size=Path(backup_path or source_path).stat().st_size,
                success=success,
                session_id=self.session_id,
                backup_path=backup_path,
                file_hash=file_hash,
                error_message=error_msg
            )
            
            if success and action == "move":
                self.statistics.moved_files += 1
                
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ç§»åŠ¨å¤±è´¥ {source_path}: {str(e)}")
            self._record_operation(
                source_path=str(source_path),
                target_path=str(target_path),
                action=action,
                file_size=0,
                success=False,
                session_id=self.session_id,
                error_message=str(e)
            )
            self.statistics.error_files += 1
            raise
    
    def _record_operation(self, source_path: str, target_path: str, action: str,
                         file_size: int, success: bool, error_message: Optional[str] = None):
        """è®°å½•æ“ä½œ"""
        operation = OrganizeOperation(
            source_path=source_path,
            target_path=target_path,
            action=action,
            file_size=file_size,
            timestamp=datetime.now(),
            success=success,
            error_message=error_message
        )
        self.operations.append(operation)
    
    def _generate_smart_plan(self, analyzed_files: List[Dict], file_paths: List[Path]) -> Dict[str, any]:
        """ç”Ÿæˆæ™ºèƒ½æ•´ç†æ–¹æ¡ˆ"""
        plan = {
            'target_directory': str(self.target_directory),
            'total_files': len(file_paths),
            'move_operations': [],
            'skip_operations': [],
            'duplicate_handling': [],
            'backup_required': 0,
            'estimated_time': '0',
            'risk_level': 'low',
            'classification_strategy': {}
        }
        
        # æ£€æµ‹é‡å¤æ–‡ä»¶
        duplicates = self._detect_duplicates(file_paths)
        duplicate_map = {}
        for group in duplicates:
            keeper, candidates = self.duplicate_detector.resolve_duplicates(group)
            for candidate in candidates:
                duplicate_map[str(candidate)] = str(keeper)
                plan['duplicate_handling'].append({
                    'duplicate': str(candidate),
                    'keeper': str(keeper),
                    'strategy': self.config.get('duplicate_strategy', 'keep_newest')
                })
        
        # ç”Ÿæˆç§»åŠ¨æ“ä½œ
        for file_analysis in analyzed_files:
            file_info = file_analysis['file_info']
            file_path = Path(file_info['path'])
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ–‡ä»¶
            if str(file_path) in duplicate_map:
                continue
            
            # ç¡®å®šç›®æ ‡è·¯å¾„
            target_path = self._determine_target_path(file_analysis)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç§»åŠ¨
            if file_path == target_path:
                plan['skip_operations'].append({
                    'source': str(file_path),
                    'reason': 'already_in_correct_location'
                })
            else:
                plan['move_operations'].append({
                    'source': str(file_path),
                    'target': str(target_path),
                    'size': file_info['size'],
                    'category': file_analysis['suggested_category'],
                    'confidence': file_analysis['confidence_score']
                })
                plan['backup_required'] += 1
        
        plan['estimated_time'] = f"{len(plan['move_operations']) * 0.1:.1f} seconds"
        plan['risk_level'] = 'high' if len(plan['move_operations']) > 100 else 'medium' if len(plan['move_operations']) > 50 else 'low'
        
        return plan
    
    def _save_plan(self, plan: Dict[str, any]) -> str:
        """ä¿å­˜æ•´ç†æ–¹æ¡ˆåˆ°æ–‡ä»¶"""
        from datetime import datetime
        import json
        
        plans_dir = Path('./plans')
        plans_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        plan_file = plans_dir / f"smart_plan_{timestamp}.json"
        
        plan['generated_at'] = datetime.now().isoformat()
        plan['session_id'] = self.session_id
        
        with open(plan_file, 'w', encoding='utf-8') as f:
            json.dump(plan, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"æ–¹æ¡ˆå·²ä¿å­˜è‡³: {plan_file}")
        return str(plan_file)
    
    def _load_plan(self, plan_file: str) -> Dict[str, any]:
        """åŠ è½½æ•´ç†æ–¹æ¡ˆ"""
        import json
        
        with open(plan_file, 'r', encoding='utf-8') as f:
            plan = json.load(f)
        
        return plan
    
    def _execute_smart_plan(self, plan: Dict[str, any], dry_run: bool):
        """æ‰§è¡Œæ™ºèƒ½æ•´ç†æ–¹æ¡ˆï¼ˆé€‚åº”ç§»åŠ¨-å¤åˆ¶å¤‡ä»½ç­–ç•¥ï¼‰"""
        self.logger.info(f"ğŸ›¡ï¸ å¼€å§‹æ‰§è¡Œæ•´ç†æ–¹æ¡ˆï¼Œå…± {len(plan['move_operations'])} ä¸ªç§»åŠ¨æ“ä½œ")
        
        # é˜¶æ®µ1ï¼šä¸ºæ‰€æœ‰éœ€è¦ç§»åŠ¨çš„æ–‡ä»¶åˆ›å»ºå¤‡ä»½ï¼ˆç§»åŠ¨åŸå§‹æ–‡ä»¶åˆ°å¤‡ä»½ä½ç½®ï¼‰
        if not dry_run:
            self.logger.info(f"ğŸ”’ é˜¶æ®µ1ï¼šç§»åŠ¨åŸå§‹æ–‡ä»¶åˆ°å¤‡ä»½ä½ç½®ï¼Œå…± {len(plan['move_operations'])} ä¸ªæ–‡ä»¶")
            backup_count = 0
            for operation in plan['move_operations']:
                source_path = Path(operation['source'])
                if source_path.exists():
                    backup_record = self.backup_manager.create_backup(
                        source_path, 
                        {
                            'operation': operation, 
                            'category': operation['category'],
                            'session_id': self.session_id,
                            'target_path': operation['target']
                        }
                    )
                    if backup_record:
                        backup_count += 1
                        if backup_count <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ªå¤‡ä»½ä¿¡æ¯
                            self.logger.debug(f"ç§»åŠ¨åˆ°å¤‡ä»½: {source_path.name} â†’ {Path(backup_record.backup_path).name}")
            self.logger.info(f"âœ… é˜¶æ®µ1å®Œæˆï¼šå·²ç§»åŠ¨ {backup_count} ä¸ªæ–‡ä»¶åˆ°å¤‡ä»½ä½ç½®")
        
        self.logger.info(f"ğŸ”’ é˜¶æ®µ1å®Œæˆï¼šå·²ä¸º {plan['backup_required']} ä¸ªæ–‡ä»¶åˆ›å»ºç§»åŠ¨å¤‡ä»½")
        
        # é˜¶æ®µ2ï¼šä»å¤‡ä»½ä½ç½®å¤åˆ¶æ–‡ä»¶åˆ°æ–°çš„ç›®æ ‡ä½ç½®
        self.logger.info(f"ğŸ”„ é˜¶æ®µ2ï¼šä»å¤‡ä»½å¤åˆ¶æ–‡ä»¶åˆ°ç›®æ ‡ä½ç½®")
        copied_count = 0
        for operation in plan['move_operations']:
            source_path = Path(operation['source'])
            target_path = Path(operation['target'])
            
            if dry_run:
                self.logger.info(f"[è¯•è¿è¡Œ] ğŸ”„ å¤åˆ¶: {source_path} â†’ {target_path}")
                self.statistics.moved_files += 1
            else:
                self._move_file(source_path, target_path, dry_run=False, action="move")
                copied_count += 1
        
        self.logger.info(f"âœ… é˜¶æ®µ2å®Œæˆï¼šå·²å¤åˆ¶ {copied_count} ä¸ªæ–‡ä»¶åˆ°ç›®æ ‡ä½ç½®")
        
        # å¤„ç†é‡å¤æ–‡ä»¶
        for dup_info in plan['duplicate_handling']:
            if not dry_run:
                duplicate_path = Path(dup_info['duplicate'])
                keeper_path = Path(dup_info['keeper'])
                self._handle_duplicate_file(duplicate_path, str(keeper_path), dry_run=False)
    
    def _create_analysis_result(self, analyzed_files: List[Dict]) -> Dict[str, any]:
        """åˆ›å»ºåˆ†æç»“æœ"""
        # ç»Ÿè®¡æ–‡ä»¶ç±»å‹åˆ†å¸ƒ
        type_stats = {}
        for file_analysis in analyzed_files:
            category = file_analysis['suggested_category']
            type_stats[category] = type_stats.get(category, 0) + 1
        
        return {
            'success': True,
            'message': 'æ–‡ä»¶åˆ†æå®Œæˆ',
            'analysis_results': {
                'total_files': len(analyzed_files),
                'type_distribution': type_stats,
                'detailed_analysis': analyzed_files
            }
        }
    
    def _create_plan_result(self, plan: Dict[str, any], plan_file: str) -> Dict[str, any]:
        """åˆ›å»ºæ–¹æ¡ˆç»“æœ"""
        return {
            'success': True,
            'message': 'æ•´ç†æ–¹æ¡ˆç”Ÿæˆå®Œæˆ',
            'plan': plan,
            'plan_file': plan_file
        }
    
    def _create_smart_result(self, success: bool, message: str, plan: Dict[str, any],
                           dry_run: bool = False) -> Dict[str, any]:
        """åˆ›å»ºæ™ºèƒ½æ•´ç†ç»“æœå­—å…¸"""
        return {
            'success': success,
            'message': message,
            'dry_run': dry_run,
            'session_id': self.session_id,
            'statistics': asdict(self.statistics),
            'operations': [asdict(op) for op in self.operations],
            'plan': plan,
            'target_directory': str(self.target_directory)
        }
    
    def _create_result_dict(self, success: bool, message: str, 
                           dry_run: bool = False) -> Dict[str, any]:
        """åˆ›å»ºç»“æœå­—å…¸ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
        return {
            'success': success,
            'message': message,
            'dry_run': dry_run,
            'statistics': asdict(self.statistics),
            'operations': [asdict(op) for op in self.operations],
            'target_directory': str(self.target_directory)
        }
    
    def get_operations_summary(self) -> Dict[str, int]:
        """è·å–æ“ä½œæ‘˜è¦ç»Ÿè®¡"""
        summary = {
            'move': 0,
            'copy': 0,
            'skip': 0,
            'duplicate': 0,
            'error': 0
        }
        
        for operation in self.operations:
            if operation.success:
                summary[operation.action] += 1
            else:
                summary['error'] += 1
        
        return summary
    
    def rollback_last_operation(self) -> bool:
        """å›æ»šæœ€åä¸€æ¬¡æ“ä½œï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
        if not self.operations:
            return False
        
        last_op = self.operations[-1]
        if not last_op.success or not last_op.target_path:
            return False
        
        try:
            # å°†æ–‡ä»¶ç§»å›åŸä½ç½®
            source = Path(last_op.target_path)
            target = Path(last_op.source_path)
            
            if source.exists() and not target.exists():
                source.rename(target)
                self.logger.info(f"å›æ»šæ“ä½œ: {source} â†’ {target}")
                self.operations.pop()  # ç§»é™¤æ“ä½œè®°å½•
                return True
                
        except Exception as e:
            self.logger.error(f"å›æ»šå¤±è´¥: {str(e)}")
        
        return False
    
    def undo_session(self, session_id: str) -> Tuple[bool, List[str]]:
        """æ’¤é”€æ•´ä¸ªä¼šè¯çš„æ“ä½œ"""
        return self.backup_manager.restore_session(session_id)
    
    def undo_file(self, session_id: str, source_path: str) -> bool:
        """æ’¤é”€å•ä¸ªæ–‡ä»¶çš„æ“ä½œ"""
        return self.backup_manager.restore_file(session_id, source_path)
    
    def list_sessions(self, status_filter: Optional[str] = None) -> List[Dict[str, any]]:
        """åˆ—å‡ºæ‰€æœ‰ä¼šè¯"""
        sessions = self.backup_manager.list_sessions(status_filter)
        return [
            {
                'session_id': s.session_id,
                'target_directory': s.target_directory,
                'start_time': s.start_time.isoformat(),
                'end_time': s.end_time.isoformat() if s.end_time else None,
                'status': s.status,
                'backup_count': len(s.backup_records),
                'operation_summary': s.operation_summary
            }
            for s in sessions
        ]
    
    def _find_backup_record(self, source_path: str) -> Optional[BackupRecord]:
        """æŸ¥æ‰¾æŒ‡å®šæºæ–‡ä»¶çš„å¤‡ä»½è®°å½•"""
        if not self.session_id:
            return None
            
        session = self.backup_manager.get_session(self.session_id)
        if not session:
            return None
            
        for record in session.backup_records:
            if record.source_path == source_path:
                return record
        return None
    
    def get_session_info(self, session_id: str) -> Optional[Dict[str, any]]:
        """è·å–ä¼šè¯è¯¦ç»†ä¿¡æ¯"""
        session = self.backup_manager.get_session(session_id)
        if not session:
            return None
        
        return {
            'session_id': session.session_id,
            'target_directory': session.target_directory,
            'start_time': session.start_time.isoformat(),
            'end_time': session.end_time.isoformat() if session.end_time else None,
            'status': session.status,
            'backup_records': [
                {
                    'source_path': r.source_path,
                    'backup_path': r.backup_path,
                    'file_size': r.file_size,
                    'backup_time': r.backup_time.isoformat(),
                    'metadata': r.metadata
                }
                for r in session.backup_records
            ],
            'operation_summary': session.operation_summary
        }


# ä¾¿æ·å‡½æ•°
def organize_directory(directory: str, config_file: Optional[str] = None, 
                      dry_run: bool = False) -> Dict[str, any]:
    """
    æ•´ç†ç›®å½•çš„ä¾¿æ·å‡½æ•°
    
    Args:
        directory: ç›®æ ‡ç›®å½•
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        dry_run: æ˜¯å¦è¯•è¿è¡Œ
        
    Returns:
        æ•´ç†ç»“æœ
    """
    organizer = FileOrganizer(directory, config_file)
    return organizer.organize(dry_run=dry_run)


def scan_for_duplicates(directory: str, config_file: Optional[str] = None) -> Dict[str, any]:
    """
    æ‰«æç›®å½•ä¸­çš„é‡å¤æ–‡ä»¶
    
    Args:
        directory: ç›®æ ‡ç›®å½•
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é‡å¤æ–‡ä»¶æ‰«æç»“æœ
    """
    organizer = FileOrganizer(directory, config_file)
    
    try:
        # åªæ‰§è¡Œæ‰«æå’Œæ£€æµ‹æ­¥éª¤
        file_paths = organizer._scan_files()
        duplicates = organizer._detect_duplicates(file_paths)
        
        stats = organizer.duplicate_detector.get_duplicate_statistics()
        
        return {
            'success': True,
            'duplicate_groups': len(duplicates),
            'total_duplicate_files': stats['total_duplicate_files'],
            'statistics': stats,
            'duplicate_details': [[str(f) for f in group] for group in duplicates]
        }
    except Exception as e:
        return {
            'success': False,
            'message': f"æ‰«æå¤±è´¥: {str(e)}"
        }